<html> 
 <head>
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8">


<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers">

  <meta name="citation_author" content="Allwein, Erin L.">

  <meta name="citation_author" content="Schapire, Robert E.">

  <meta name="citation_author" content="Singer, Yoram">

<meta name="citation_publication_date" content="2000">
<meta name="citation_journal_title" content="Journal of Machine Learning Research">
<meta name="citation_issn" content="ISSN 1533-7928">
<meta name="citation_volume" content="1">
<meta name="citation_issue" content="Dec">
<meta name="citation_firstpage" content="113">
<meta name="citation_lastpage" content="141">
<meta name="citation_pdf_url" content="http://www.jmlr.org/papers/volume1/allwein00a/allwein00a.pdf">
 
  <!--#include virtual="/css-scroll.txt"-->
<style>
. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
</style>
</head>
<body>
<div id="content">
<h2>Reducing Multiclass to Binary: A Unifying Approach for Margin Classifiers</h2>
<p><b><i>Erin L. Allwein, Robert E. Schapire, Yoram Singer</b></i>; 
1(Dec):113-141, 2000.</p>
<h3>Abstract</h3>
We present a unifying framework for studying the solution of
multiclass categorization problems by reducing them to multiple binary
problems that are then solved using a margin-based binary learning
algorithm.  The proposed framework unifies some of the most popular
approaches in which each class is compared against all others, or in
which all pairs of classes are compared to each other, or in which
output codes with error-correcting properties are used.  We propose a
general method for combining the classifiers generated on the binary
problems, and we prove a general empirical <em>multiclass</em> loss
bound given the empirical loss of the individual <em>binary</em> learning
algorithms.  The scheme and the corresponding bounds apply to many
popular classification learning algorithms including support-vector
machines, AdaBoost, regression, logistic regression and decision-tree
algorithms.  We also give a multiclass generalization error analysis
for general output codes with AdaBoost as the binary learner.
Experimental results with SVM and AdaBoost show that our scheme
provides a viable alternative to the most commonly used multiclass
algorithms.
<p><font color=gray>[abs]</font>
<a target=_blank href=http://www.jmlr.org/papers/volume1/allwein00a/allwein00a.pdf>[pdf]</a>
<a target=_blank href=http://www.jmlr.org/papers/volume1/allwein00a/allwein00a.ps.gz>[ps.gz]</a>
<a target=_blank href=http://www.jmlr.org/papers/volume1/allwein00a/allwein00a.ps>[ps]</a>
<img src=http://www.jmlr.org/images/1p-trans.gif height=1 width=24><a target=_blank href=http://citeseer.nj.nec.com/382580.html>[citations]</a>
</div> 
 <!--#include virtual="/nav-bar.txt"--> 
  </body>
 </html>
