<html> 
 <head>
  <meta http-equiv="Content-type" content="text/html;charset=UTF-8">


<!-- Google Scholar Meta Data -->

<meta name="citation_title" content="Bayes Point Machines">

  <meta name="citation_author" content="Herbrich, Ralf">

  <meta name="citation_author" content="Graepel, Thore">

  <meta name="citation_author" content="Campbell, Colin">

<meta name="citation_publication_date" content="2001">
<meta name="citation_journal_title" content="Journal of Machine Learning Research">
<meta name="citation_issn" content="ISSN 1533-7928">
<meta name="citation_volume" content="1">
<meta name="citation_issue" content="Aug">
<meta name="citation_firstpage" content="245">
<meta name="citation_lastpage" content="279">
<meta name="citation_pdf_url" content="http://www.jmlr.org/papers/volume1/herbrich01a/herbrich01a.pdf">
 
  <!--#include virtual="/css-scroll.txt"-->
<style>
. {font-family:verdana,helvetica,sans-serif}
a {text-decoration:none;color:#3030a0}
 </style>
<body>
 <div id="content">

<h2>Bayes Point Machines</h2>
<p><b><i>Ralf Herbrich, Thore Graepel, Colin Campbell</b></i>; 
1(Aug):245-279, 2001.</p>
<h3>Abstract</h3>
Kernel-classifiers comprise a powerful class of non-linear decision
functions for binary classification. The support vector machine is an
example of a learning algorithm for kernel classifiers that singles
out the consistent classifier with the largest margin, i.e. minimal
real-valued output on the training sample, within the set of
consistent hypotheses, the so-called <i>version space</i>. We suggest
the <i>Bayes point machine</i> as a well-founded improvement which
approximates the Bayes-optimal decision by the centre of mass of
version space. We present two algorithms to stochastically approximate
the centre of mass of version space: a billiard sampling algorithm and
a sampling algorithm based on the well known perceptron algorithm. It
is shown how both algorithms can be extended to allow for
soft-boundaries in order to admit training errors.  Experimentally, we
find that - for the zero training error case - Bayes point
machines consistently outperform support vector machines on both
surrogate data and real-world benchmark data sets. In the
soft-boundary/soft-margin case, the improvement over support vector
machines is shown to be reduced. Finally, we demonstrate that the
real-valued output of single Bayes points on novel test points is a
valid <i>confidence</i> measure and leads to a steady decrease in
generalisation error when used as a rejection criterion.
<p><font color=gray>[abs]</font>
<a target=_blank href=http://www.jmlr.org/papers/volume1/herbrich01a/herbrich01a.pdf>[pdf]</a>
<a target=_blank href=http://www.jmlr.org/papers/volume1/herbrich01a/herbrich01a.ps.gz>[ps.gz]</a>
<a target=_blank href=http://www.jmlr.org/papers/volume1/herbrich01a/herbrich01a.ps>[ps]</a>
</div> 
 <!--#include virtual="/nav-bar.txt"--> 
  </body>
 </html>
